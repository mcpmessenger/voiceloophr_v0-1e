# VoiceLoopHR Upgrade Instructions

This document provides comprehensive instructions for upgrading the VoiceLoopHR application to integrate OpenAI Whisper for speech-to-text (STT) and ElevenLabs for text-to-speech (TTS) capabilities. These upgrades will replace the currently simulated STT and TTS functionalities with real-world API integrations, enhancing the application's core features.

## Table of Contents

1.  [Introduction](#introduction)
2.  [Prerequisites](#prerequisites)
3.  [OpenAI Whisper Integration](#openai-whisper-integration)
    *   [Understanding the Current STT Implementation](#understanding-the-current-stt-implementation)
    *   [Modifying the STT API Route](#modifying-the-stt-api-route)
    *   [Handling Audio Files](#handling-audio-files)
4.  [ElevenLabs Integration](#elevenlabs-integration)
    *   [Understanding the Current TTS Implementation](#understanding-the-current-tts-implementation)
    *   [Modifying the TTS API Route](#modifying-the-tts-api-route)
    *   [Handling Audio Playback](#handling-audio-playback)
5.  [Frontend Modifications](#frontend-modifications)
6.  [Testing and Validation](#testing-and-validation)
7.  [Troubleshooting](#troubleshooting)
8.  [Conclusion](#conclusion)

## 1. Introduction

The VoiceLoopHR application currently uses simulated responses for its speech-to-text and text-to-speech functionalities. This guide will walk you through the process of integrating actual APIs from OpenAI (Whisper) and ElevenLabs to provide robust and accurate voice processing capabilities. This upgrade is crucial for the application's transition from a prototype to a fully functional system capable of real-time voice interactions.




## 2. Prerequisites

Before proceeding with the integration, ensure you have the following:

*   **Node.js and npm/pnpm:** The VoiceLoopHR application is built with Next.js, which requires Node.js. Ensure you have a recent version installed. pnpm is used for package management in this repository.
*   **OpenAI API Key:** You will need an API key from OpenAI to access the Whisper API. Obtain one from the [OpenAI platform](https://platform.openai.com/account/api-keys).
*   **ElevenLabs API Key:** Similarly, an API key from ElevenLabs is required for text-to-speech services. You can obtain this from your [ElevenLabs account](https://elevenlabs.io/account/history).
*   **Basic Understanding of Next.js:** Familiarity with Next.js API routes and frontend development will be beneficial.
*   **Git:** For cloning the repository and managing code versions.

To set up the project locally, clone the repository and install dependencies:

```bash
git clone https://github.com/peercodeai/voiceloophr_v0-1e
cd voiceloophr_v0-1e
pnpm install
```





## 3. OpenAI Whisper Integration

### Understanding the Current STT Implementation

The current speech-to-text (STT) functionality in VoiceLoopHR is located in `app/api/stt/route.ts`. As observed during the repository analysis, this file contains a simulated transcription process. The relevant section is commented out, showing the intended real implementation using the OpenAI Whisper API.

```typescript
// app/api/stt/route.ts

import { type NextRequest, NextResponse } from "next/server"

export async function POST(request: NextRequest) {
  try {
    const formData = await request.formData()
    const audioFile = formData.get("audio") as File
    const openaiKey = formData.get("openaiKey") as string

    if (!audioFile || !openaiKey) {
      return NextResponse.json({ error: "Missing audio file or OpenAI key" }, { status: 400 })
    }

    // In a real implementation, this would call OpenAI Whisper API
    // For now, simulate speech-to-text
    await new Promise((resolve) => setTimeout(resolve, 1500))

    const simulatedTranscription =
      "This is a simulated transcription of your voice input. In a real implementation, this would use OpenAI's Whisper API to convert your speech to text with high accuracy."

    return NextResponse.json({
      success: true,
      transcription: simulatedTranscription,
    })

    /* Real implementation would be:
    const whisperFormData = new FormData()
    whisperFormData.append(\'file\', audioFile)
    whisperFormData.append(\'model\', \'whisper-1\')

    const response = await fetch(\'https://api.openai.com/v1/audio/transcriptions\', {
      method: \'POST\',
      headers: {
        \'Authorization\': `Bearer ${openaiKey}`
      },
      body: whisperFormData
    })

    if (!response.ok) {
      throw new Error(\'Speech-to-text failed\')
    }

    const result = await response.json()
    return NextResponse.json({ success: true, transcription: result.text })
    */
  } catch (error) {
    console.error("STT error:", error)
    return NextResponse.json({ error: "Speech-to-text failed" }, { status: 500 })
  }
}
```

### Modifying the STT API Route

To enable the actual Whisper integration, you need to uncomment and utilize the provided real implementation block within `app/api/stt/route.ts`. This involves sending the audio file to OpenAI's `/v1/audio/transcriptions` endpoint.

1.  **Open `app/api/stt/route.ts`** in your editor.

2.  **Replace the simulated transcription logic** with the commented-out real implementation. Your `POST` function should look like this:

    ```typescript
    // app/api/stt/route.ts

    import { type NextRequest, NextResponse } from "next/server"

    export async function POST(request: NextRequest) {
      try {
        const formData = await request.formData()
        const audioFile = formData.get("audio") as File
        const openaiKey = formData.get("openaiKey") as string

        if (!audioFile || !openaiKey) {
          return NextResponse.json({ error: "Missing audio file or OpenAI key" }, { status: 400 })
        }

        const whisperFormData = new FormData()
        whisperFormData.append('file', audioFile)
        whisperFormData.append('model', 'whisper-1')

        const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${openaiKey}`
          },
          body: whisperFormData
        })

        if (!response.ok) {
          const errorData = await response.json();
          throw new Error(`Speech-to-text failed: ${errorData.error.message}`)
        }

        const result = await response.json()
        return NextResponse.json({ success: true, transcription: result.text })
      } catch (error) {
        console.error("STT error:", error)
        return NextResponse.json({ error: `Speech-to-text failed: ${(error as Error).message}` }, { status: 500 })
      }
    }
    ```

    **Note:** I've added a small enhancement to the error handling to include the specific error message from OpenAI's API response, which will be helpful for debugging.

### Handling Audio Files

The Whisper API expects audio files in various formats (M4A, MP3, WAV, FLAC, etc.). The current frontend likely sends the audio as a `File` object within `FormData`, which is compatible with the Whisper API. Ensure that the audio recording mechanism in the frontend (e.g., in `app/chat/page.tsx` or related components) correctly captures and sends the audio data.





## 4. ElevenLabs Integration

### Understanding the Current TTS Implementation

Similar to the STT functionality, the text-to-speech (TTS) implementation in `app/api/tts/route.ts` is currently simulated. The commented-out section provides the blueprint for integrating with the ElevenLabs API.

```typescript
// app/api/tts/route.ts

import { type NextRequest, NextResponse } from "next/server"

export async function POST(request: NextRequest) {
  try {
    const { text, elevenlabsKey } = await request.json()

    if (!text || !elevenlabsKey) {
      return NextResponse.json({ error: "Missing text or ElevenLabs key" }, { status: 400 })
    }

    // In a real implementation, this would call ElevenLabs API
    // For now, simulate TTS processing
    await new Promise((resolve) => setTimeout(resolve, 1000))

    // Simulate audio generation
    const audioUrl = `/placeholder-audio.mp3?text=${encodeURIComponent(text.substring(0, 50))}`

    return NextResponse.json({
      success: true,
      audioUrl,
      text: text.substring(0, 100) + (text.length > 100 ? "..." : ""),
    })

    /* Real implementation would be:
    const response = await fetch(\'https://api.elevenlabs.io/v1/text-to-speech/21m00Tcm4TlvDq8ikWAM\', {
      method: \'POST\',
      headers: {
        \'Accept\': \'audio/mpeg\',
        \'Content-Type\': \'application/json\',
        \'xi-api-key\': elevenlabsKey
      },
      body: JSON.stringify({
        text,
        model_id: \'eleven_monolingual_v1\',
        voice_settings: {
          stability: 0.5,
          similarity_boost: 0.5
        }
      })
    })

    if (!response.ok) {
      throw new Error(\'TTS generation failed\')
    }

    const audioBuffer = await response.arrayBuffer()
    const audioBlob = new Blob([audioBuffer], { type: \'audio/mpeg\' })
    
    // In a real app, you\'d save this to storage and return a URL
    return NextResponse.json({ success: true, audioUrl: \'generated-audio-url\' })
    */
  } catch (error) {
    console.error("TTS error:", error)
    return NextResponse.json({ error: "TTS generation failed" }, { status: 500 })
  }
}
```

### Modifying the TTS API Route

To integrate ElevenLabs, you will replace the simulated TTS logic with the actual API call. The ElevenLabs API returns an audio stream, which you'll need to handle and potentially convert into a playable format or a URL that the frontend can consume.

1.  **Open `app/api/tts/route.ts`** in your editor.

2.  **Replace the simulated TTS logic** with the commented-out real implementation. Your `POST` function should be updated as follows:

    ```typescript
    // app/api/tts/route.ts

    import { type NextRequest, NextResponse } from "next/server"

    export async function POST(request: NextRequest) {
      try {
        const { text, elevenlabsKey } = await request.json()

        if (!text || !elevenlabsKey) {
          return NextResponse.json({ error: "Missing text or ElevenLabs key" }, { status: 400 })
        }

        const response = await fetch("https://api.elevenlabs.io/v1/text-to-speech/21m00Tcm4TlvDq8ikWAM", {
          method: "POST",
          headers: {
            "Accept": "audio/mpeg",
            "Content-Type": "application/json",
            "xi-api-key": elevenlabsKey
          },
          body: JSON.stringify({
            text,
            model_id: "eleven_monolingual_v1", // Or choose another model like "eleven_multilingual_v2"
            voice_settings: {
              stability: 0.5,
              similarity_boost: 0.5
            }
          })
        })

        if (!response.ok) {
          const errorData = await response.json();
          throw new Error(`TTS generation failed: ${errorData.detail}`)
        }

        // ElevenLabs returns an audio stream directly. We can pipe this directly to the client.
        return new NextResponse(response.body, {
          headers: {
            "Content-Type": "audio/mpeg",
            "Content-Disposition": "inline; filename=\"audio.mp3\"",
          },
        });

      } catch (error) {
        console.error("TTS error:", error)
        return NextResponse.json({ error: `TTS generation failed: ${(error as Error).message}` }, { status: 500 })
      }
    }
    ```

    **Note:** I've modified the response handling to directly stream the audio from ElevenLabs to the client. This is a more efficient way to handle audio responses in Next.js API routes. Also, improved error handling to capture specific error messages from ElevenLabs.

### Handling Audio Playback

With the updated TTS API route, the frontend will receive an `audio/mpeg` stream directly. The existing frontend code that plays the `audioUrl` will likely need to be adjusted to handle this direct stream. Instead of receiving a URL, it will receive the audio data directly. You might need to create a `Blob` from the response and then a `URL` for an `<audio>` element, or directly use the `Response` object if the frontend is set up to consume streams.

For example, in your frontend component (e.g., `app/chat/page.tsx` or where the TTS audio is played):

```typescript
// Example of frontend adjustment (conceptual)

const handlePlayTTS = async (text: string, elevenlabsKey: string) => {
  try {
    const response = await fetch('/api/tts', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ text, elevenlabsKey }),
    });

    if (!response.ok) {
      throw new Error('Failed to get audio from TTS API');
    }

    const audioBlob = await response.blob();
    const audioUrl = URL.createObjectURL(audioBlob);
    const audio = new Audio(audioUrl);
    audio.play();

    audio.onended = () => {
      URL.revokeObjectURL(audioUrl); // Clean up the object URL after playback
    };

  } catch (error) {
    console.error('Error playing TTS:', error);
  }
};
```





## 5. Frontend Modifications

The primary frontend modifications will involve ensuring that the API keys are correctly passed to the backend and handling the audio responses. The existing code already passes `openaiKey` and `elevenlabsKey` in the `formData` or request body, which is good.

### Passing API Keys

Verify that your frontend components (e.g., `app/chat/page.tsx` or any component responsible for making API calls to `/api/stt` and `/api/tts`) are securely obtaining and passing the `openaiKey` and `elevenlabsKey` to the backend. These keys should ideally be managed as environment variables or through a secure client-side mechanism, rather than being hardcoded.

### Handling Audio Playback for TTS

As discussed in the ElevenLabs integration section, the `app/api/tts/route.ts` now directly streams the audio. You will need to adjust the frontend to handle this. The conceptual example provided earlier demonstrates how to convert the incoming audio stream into a `Blob` and then create an object URL for playback. Ensure your actual implementation reflects this change.

Look for code similar to this in your frontend (e.g., in `app/chat/page.tsx`):

```typescript
// Example of how the frontend might call the STT API
const handleSpeechToText = async (audioBlob: Blob, openaiKey: string) => {
  const formData = new FormData();
  formData.append("audio", audioBlob);
  formData.append("openaiKey", openaiKey);

  const response = await fetch("/api/stt", {
    method: "POST",
    body: formData,
  });

  if (!response.ok) {
    // Handle error
    return;
  }
  const data = await response.json();
  console.log("Transcription:", data.transcription);
};

// Example of how the frontend might call the TTS API and play audio
const handleTextToSpeech = async (text: string, elevenlabsKey: string) => {
  const response = await fetch("/api/tts", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    body: JSON.stringify({ text, elevenlabsKey }),
  });

  if (!response.ok) {
    // Handle error
    return;
  }

  // This part needs adjustment for streaming audio
  const audioBlob = await response.blob();
  const audioUrl = URL.createObjectURL(audioBlob);
  const audio = new Audio(audioUrl);
  audio.play();

  audio.onended = () => {
    URL.revokeObjectURL(audioUrl);
  };
};
```

Ensure that the `handleTextToSpeech` function (or its equivalent in your codebase) correctly processes the `response.blob()` and plays the audio. The `URL.createObjectURL` and `URL.revokeObjectURL` pattern is important for memory management.





## 6. Testing and Validation

After implementing the changes, thorough testing is crucial to ensure that both the Whisper STT and ElevenLabs TTS integrations are working as expected.

### Local Testing

1.  **Start the Development Server:**
    ```bash
    pnpm dev
    ```
2.  **Access the Application:** Open your browser and navigate to `http://localhost:3000` (or whatever port your Next.js app is running on).
3.  **Test STT:** Use the application's voice input feature (e.g., in the chat interface). Speak clearly and observe the transcription results. Verify that the transcribed text accurately reflects your speech.
4.  **Test TTS:** Trigger the application's text-to-speech output. Listen carefully to the generated audio. Check for clarity, naturalness, and accuracy of the spoken text.
5.  **Monitor Console and Network:** Keep your browser's developer console open (F12) to check for any errors in the network requests to `/api/stt` and `/api/tts`, as well as any console logs from your Next.js application.

### Error Handling Verification

*   **Missing API Keys:** Test the application's behavior when API keys are missing or invalid. The backend should return appropriate 400 or 401 errors.
*   **API Rate Limits/Errors:** While difficult to simulate directly, be aware of potential rate limits or other API errors from OpenAI or ElevenLabs. Ensure your application handles these gracefully (e.g., by displaying user-friendly error messages).

### Performance Considerations

*   **Latency:** Observe the time it takes for speech to be transcribed and for text to be converted to speech. Real-time applications require low latency. If latency is high, consider optimizing audio processing or exploring different API models.
*   **Audio Quality:** For STT, ensure the input audio quality is good. For TTS, evaluate the output audio quality and naturalness. ElevenLabs offers various voice models and settings (e.g., `stability`, `similarity_boost`) that can be fine-tuned for better results.





## 7. Troubleshooting

Here are some common issues you might encounter during the integration process and how to resolve them:

*   **"Missing audio file or OpenAI key" (STT) / "Missing text or ElevenLabs key" (TTS) errors:**
    *   **Cause:** The frontend is not sending the `audio` file or the respective API keys (`openaiKey`, `elevenlabsKey`) to the backend API routes.
    *   **Solution:** Double-check your frontend code (e.g., `app/chat/page.tsx`) to ensure that `FormData` for STT and the JSON body for TTS are correctly populated with the necessary data before making the `fetch` request.

*   **"Speech-to-text failed" / "TTS generation failed" errors:**
    *   **Cause:** These are generic errors from the backend. The actual cause could be an invalid API key, an issue with the audio file format (for STT), an incorrect request body, or an external API service issue.
    *   **Solution:**
        *   **Check API Keys:** Verify that your OpenAI and ElevenLabs API keys are correct and active. Ensure they are being passed correctly from the frontend to the backend.
        *   **Inspect Backend Logs:** Look at the console output of your Next.js development server. The `console.error` statements in `app/api/stt/route.ts` and `app/api/tts/route.ts` will provide more specific error messages from the OpenAI or ElevenLabs APIs.
        *   **Network Tab:** Use your browser's developer tools (Network tab) to inspect the requests to `/api/stt` and `/api/tts`. Check the request payload and the response from the server for any error details.
        *   **Audio File Issues (STT):** Ensure the audio file being sent is a valid audio format supported by Whisper (e.g., MP3, WAV, M4A). If you're recording audio in the browser, verify the recording process is producing a valid audio blob.
        *   **ElevenLabs Voice ID:** For ElevenLabs, ensure the `voice_id` (e.g., `21m00Tcm4TlvDq8ikWAM`) in your `app/api/tts/route.ts` is valid and corresponds to a voice available in your ElevenLabs account.

*   **No audio playback after TTS:**
    *   **Cause:** The frontend is not correctly handling the audio stream returned by the `app/api/tts` route.
    *   **Solution:** Revisit the frontend code responsible for playing the audio. Ensure you are creating a `Blob` from the `response.blob()` and then creating an object URL (`URL.createObjectURL`) for the `<audio>` element. Also, make sure the `audio.play()` method is being called and there are no JavaScript errors preventing playback.

*   **High Latency:**
    *   **Cause:** Network delays, large audio file sizes, or complex processing on the API side.
    *   **Solution:**
        *   **Optimize Audio (STT):** For STT, try to send smaller chunks of audio if your application supports real-time streaming. Ensure audio is compressed appropriately before sending.
        *   **Choose Faster Models (TTS):** ElevenLabs offers different models; some might be faster than others. Experiment with `model_id` if performance is critical.
        *   **Geographic Proximity:** If possible, ensure your server (where the Next.js API routes run) is geographically close to the OpenAI and ElevenLabs API endpoints to minimize network latency.





## 8. Conclusion

By following these instructions, you will have successfully upgraded the VoiceLoopHR application to utilize the powerful OpenAI Whisper for accurate speech-to-text transcription and ElevenLabs for natural-sounding text-to-speech generation. These integrations significantly enhance the application's capabilities, moving it from a simulated environment to a real-world voice-enabled platform. Remember to thoroughly test all functionalities after implementing the changes and refer to the troubleshooting section for any issues encountered.

This upgrade lays the groundwork for more advanced voice interactions and features within VoiceLoopHR, providing a more robust and engaging user experience.


